{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx320DtZEUHF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Importing Necessary packages and mounting drive\n",
        "#Installing the necessary package for analysis and visualization of EEG data\n",
        "#!pip install mne\n",
        "#Mount drive for accessing the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Importing the necessary packages\n",
        "!pip install mne\n",
        "import mne"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Pickle Data\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/sub_data.pkl', 'rb') as file:\n",
        "    sub_data = pickle.load(file)"
      ],
      "metadata": {
        "id": "7DqH6VcxHJkj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzlfB1KLEpeg"
      },
      "outputs": [],
      "source": [
        "#@title Loading The Data for Each Subject - Method1\n",
        "#Subjects^\n",
        "\n",
        "s=['23','24','25','26','28','30','31','33','34','35','36','37','41','42']\n",
        "#Channels of interest\n",
        "ch = ['O1','Oz','O2','PO7','PO3','POz','PO4','PO8','Pz','P3','P4','P2','P1']\n",
        "import numpy as np\n",
        "sub_trials = {}\n",
        "sub_data = {}\n",
        "# Defining conditions\n",
        "conditions = [\"Stimulus/Neutral/Left/Kick\", \"Stimulus/Neutral/Left/Walk\", \"Stimulus/Neutral/Right/Kick\", \"Stimulus/Neutral/Right/Walk\",\n",
        "                  \"Stimulus/Congruent/Left/Kick\", \"Stimulus/Congruent/Left/Walk\", \"Stimulus/Congruent/Right/Kick\", \"Stimulus/Congruent/Right/Walk\",\n",
        "                  \"Stimulus/Incongruent/Left/Kick\", \"Stimulus/Incongruent/Left/Walk\", \"Stimulus/Incongruent/Right/Kick\", \"Stimulus/Incongruent/Right/Walk\"]\n",
        "#Defining the trial type id_s\n",
        "neutral_walk_ids = [31,33]\n",
        "neutral_kick_ids = [32,34]\n",
        "cong_walk_ids = [11,13]\n",
        "cong_kick_ids = [12,14]\n",
        "incong_walk_ids = [21,23]\n",
        "incong_kick_ids = [22,24]\n",
        "kick_ids = [12,14,22,24,32,34]\n",
        "walk_ids = [11,13,21,23,31,33]\n",
        "for sub_id in s:\n",
        "    fname = f\"/content/drive/MyDrive/EE585_Data/s{sub_id}_epochs_autoreject-epo.fif.gz\"\n",
        "    raw=mne.read_epochs(fname, preload=True)\n",
        "    # Extracting epochs for each condition\n",
        "    raw_epochs = raw[conditions].pick_channels(ch)\n",
        "    neutral_epochs = raw[conditions[:4]].pick_channels(ch)\n",
        "    cong_epochs = raw[conditions[4:8]].pick_channels(ch)\n",
        "    incong_epochs = raw[conditions[8:]].pick_channels(ch)\n",
        "    neutral_epoch_events = neutral_epochs.events[:, -1]\n",
        "    cong_epoch_events = cong_epochs.events[:,-1]\n",
        "    incong_epoch_events = incong_epochs.events[:,-1]\n",
        "    raw_epoch_events = raw_epochs.events[:,-1]\n",
        "    #labels: 0 for kick, 1 for walk\n",
        "    neutral_labels = np.array([0 if event_id in neutral_kick_ids else 1 for event_id in neutral_epoch_events])\n",
        "    cong_labels = np.array([0 if event_id in cong_kick_ids else 1 for event_id in cong_epoch_events])\n",
        "    incong_labels = np.array([0 if event_id in incong_kick_ids else 1 for event_id in incong_epoch_events])\n",
        "    raw_labels = np.array([0 if event_id in kick_ids else 1 for event_id in raw_epoch_events])\n",
        "    sub_data[sub_id] = {\n",
        "        'raw': raw,\n",
        "        'raw_epochs': raw_epochs,\n",
        "        'neutral_epochs': neutral_epochs,\n",
        "        'cong_epochs': cong_epochs,\n",
        "        'incong_epochs': incong_epochs,\n",
        "        'raw_epoch_events':raw_epoch_events,\n",
        "        'neutral_epoch_events':neutral_epoch_events,\n",
        "        'cong_epoch_events':cong_epoch_events,\n",
        "        'incong_epoch_events':incong_epoch_events,\n",
        "        'neutral_labels': neutral_labels,\n",
        "        'cong_labels': cong_labels,\n",
        "        'incong_labels':incong_labels,\n",
        "        'raw_labels':raw_labels\n",
        "    }\n",
        "    #_1 and _2:left & _3 and _4: right but irrelevant to our task now...\n",
        "    # Store trial information in sub_trials\n",
        "    sub_trials[sub_id] = [\n",
        "        len(neutral_epochs), np.sum(neutral_labels == 0), np.sum(neutral_labels == 1),\n",
        "        len(cong_epochs), np.sum(cong_labels == 0), np.sum(cong_labels == 1),\n",
        "        len(incong_epochs), np.sum(incong_labels == 0), np.sum(incong_labels == 1)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizations of Time-Series Data\n",
        "import matplotlib.pyplot as plt\n",
        "#for sub_id in s:\n",
        "times = np.linspace(0.1, 0.9, 8)\n",
        "#[\"aud/left\"].plot_topomap(ch_type=\"mag\", times=times, colorbar=True)\n",
        "raw.plot(picks=ch)\n",
        "raw_epochs.plot_image(picks='POz', combine=\"mean\")"
      ],
      "metadata": {
        "id": "CoE0ipHgUeaj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRRUvrrHObFI",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d994bff8-8e5c-4afa-e752-bca22842f979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subj,Neut,NK, NW ,Cong, CK ,CW,Incon,IK,IW\n",
            "31 [76, 37, 39, 265, 127, 138, 81, 37, 44]\n"
          ]
        }
      ],
      "source": [
        "#@title Number of Trial Types for Each Subject\n",
        "for i in sub_trials.keys():\n",
        "  print(\"Subj,Neut,NK, NW ,Cong, CK ,CW,Incon,IK,IW\")\n",
        "  print(i,sub_trials[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8syYyRoXxr6o"
      },
      "outputs": [],
      "source": [
        "#@title Resampling and Visualizing Time Series Data\n",
        "neutrals = 35\n",
        "cong = 125\n",
        "incong = 35\n",
        "s=['23','24','25','26','28','30','31','33','34','35','36','37','41','42']\n",
        "#Channels of interest\n",
        "ch = ['O1','Oz','O2','PO7','PO3','POz','PO4','PO8','Pz','P3','P4','P2','P1']\n",
        "#Make sampling rate 500\n",
        "for sub_id in s:\n",
        "    sub_data[sub_id]['raw'].resample(500)\n",
        "    #Further inspection of the epoched data with 65 channels and time points ranging from -2.4 to .999\n",
        "    sub_data[sub_id]['raw'].average().plot_image()\n",
        "    #0 point corresponds to target stimulus onset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO1-HVqdJPlU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Computing The Time Frequency using Morlet\n",
        "import mne\n",
        "from mne.time_frequency import tfr_array_morlet\n",
        "import numpy as np\n",
        "def time_freq(X,decim=3,n_cycles = 0.5):\n",
        "  # Define frequencies of interest (log-spaced)\n",
        "  # Define frequencies of interest for specific bands\n",
        "  alpha_frequencies = np.linspace(10, 13, num=1)  # Alpha band\n",
        "  beta_frequencies = np.linspace(13, 30, num=3)  # Beta band\n",
        "  frequencies = np.concatenate([alpha_frequencies,beta_frequencies])\n",
        "  # Compute time-frequency representation with Morlet wavelets\n",
        "  power = tfr_array_morlet(X, sfreq=500,freqs=frequencies, n_cycles=n_cycles, use_fft=True, decim=decim,output='power')\n",
        "  return power\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reshaping & Get Trial Indices\n",
        "import numpy as np\n",
        "\n",
        "#Assuming time-freq data is 4D\n",
        "def convert_2d(X):\n",
        "  n_trials, n_chans, n_freqs, n_timesteps = X.shape\n",
        "  X_2d = X.reshape(n_trials,n_chans*n_freqs*n_timesteps)\n",
        "  return X_2d\n",
        "#Assuming time-series data is 3D\n",
        "def convert_2d_ts(X):\n",
        "  n_trials, n_chans, n_timesteps = X.shape\n",
        "  X_2d = X.reshape(n_trials,n_chans*n_timesteps)\n",
        "  return X_2d\n",
        "\n",
        "def get_trial_idx(kick_list,walk_list,n_trials):\n",
        "  np.random.shuffle(kick_list)\n",
        "  kick = kick_list[(len(kick_list)-n_trials):]\n",
        "  np.random.shuffle(walk_list)\n",
        "  walk = walk_list[(len(walk_list)-n_trials):]\n",
        "  return kick, walk"
      ],
      "metadata": {
        "id": "epdMvEgRXxPx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3ZsbqF9h0lc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Normalization\n",
        "import numpy as np\n",
        "# Calculate the mean and standard deviation along the first axis (axis=0)\n",
        "# Subtract mean (zero-centered), divide by std\n",
        "def normalize(X):\n",
        "  means = np.mean(X, axis=0)\n",
        "  std_devs = np.std(X, axis=0)\n",
        "\n",
        "  # Avoid division by zero\n",
        "  std_devs[std_devs == 0] = 1\n",
        "\n",
        "  # Normalize the data\n",
        "  normalized_power_train = (X - means) / std_devs\n",
        "  return normalized_power_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPMpAZg_Pesd"
      },
      "outputs": [],
      "source": [
        "#@title Preparing training and test datasets for each subject\n",
        "from mne.time_frequency import tfr_array_morlet\n",
        "\n",
        "n_trials = 35\n",
        "c_trials = 125\n",
        "i_trials = 35\n",
        "fold = 4\n",
        "sub_val_data = {}\n",
        "sub_data_processed = {}\n",
        "for sub_id in s:\n",
        "  # Discard extra kick/walk trials in neutral ones\n",
        "  neutral_kick_indices, neutral_walk_indices = get_trial_idx(np.where(sub_data[sub_id]['neutral_labels'] == 0)[0],np.where(sub_data[sub_id]['neutral_labels'] == 1)[0],35)\n",
        "  #Discard extra kick/walk trials in incongruent ones\n",
        "  incong_kick_indices, incong_walk_indices = get_trial_idx(np.where(sub_data[sub_id]['incong_labels'] == 0)[0],np.where(sub_data[sub_id]['incong_labels'] == 1)[0],35)\n",
        "  #Discard extra kick/walk trials in congruent ones\n",
        "  cong_kick_indices, cong_walk_indices = get_trial_idx(np.where(sub_data[sub_id]['cong_labels'] == 0)[0],np.where(sub_data[sub_id]['cong_labels'] == 1)[0],125)\n",
        "\n",
        "  #Find the trial indices to split into training and test sets (80%-20%)\n",
        "  train_indices = []\n",
        "  test_indices = []\n",
        "  train_indices = np.concatenate([neutral_kick_indices[:28],neutral_walk_indices[:28],incong_kick_indices[:28],incong_walk_indices[:28],cong_kick_indices[:100],cong_walk_indices[:100]])\n",
        "  #val_indices = np.concatenate([neutral_kick_indices[:7],neutral_walk_indices[:7],incong_kick_indices[:7],incong_walk_indices[:7],cong_kick_indices[:25],cong_walk_indices[:25]])\n",
        "  #train_val_indices = np.concatenate([neutral_kick_indices[7:28],neutral_walk_indices[7:28],incong_kick_indices[7:28],incong_walk_indices[7:28],cong_kick_indices[25:100],cong_walk_indices[15:100]])\n",
        "  test_indices = np.concatenate([neutral_kick_indices[28:],neutral_walk_indices[28:],incong_kick_indices[28:],incong_walk_indices[28:],cong_kick_indices[100:],cong_walk_indices[100:]])\n",
        "  sub_val_data[sub_id] = {\n",
        "        'x_val0': [],\n",
        "        'x_val1': [],\n",
        "        'x_val2': [],\n",
        "        'x_val3': [],\n",
        "        'y_val0': [],\n",
        "        'y_val1': [],\n",
        "        'y_val2': [],\n",
        "        'y_val3': [],\n",
        "        'x_norm_val_power2d0': [],\n",
        "        'x_norm_val_power2d1': [],\n",
        "        'x_norm_val_power2d2': [],\n",
        "        'x_norm_val_power2d3': [],\n",
        "        'x_norm_train_power2d0': [],\n",
        "        'x_norm_train_power2d1': [],\n",
        "        'x_norm_train_power2d2': [],\n",
        "        'x_norm_train_power2d3': [],\n",
        "        'y_train0': [],\n",
        "        'y_train1': [],\n",
        "        'y_train2': [],\n",
        "        'y_train3': [],\n",
        "        'x_train0':[],\n",
        "        'x_train1':[],\n",
        "        'x_train2':[],\n",
        "        'x_train3':[]\n",
        "    }\n",
        "\n",
        "  for f in range(fold):\n",
        "    np.random.shuffle(train_indices)\n",
        "    #Extract training and validation data (time-series), from 100ms - 700ms\n",
        "    sub_val_data[sub_id][f'x_val{str(f)}'] = sub_data[sub_id]['raw_epochs'].crop(tmin=0.1, tmax=0.7).get_data(copy=False)[train_indices[:78]]\n",
        "    sub_val_data[sub_id][f'y_val{str(f)}'] = sub_data[sub_id]['raw_labels'][train_indices[:78]]\n",
        "    sub_val_data[sub_id][f'x_norm_val_power2d{str(f)}'] = convert_2d(normalize(time_freq(sub_val_data[sub_id][f'x_val{str(f)}'])))\n",
        "    sub_val_data[sub_id][f'x_train{str(f)}'] = sub_data[sub_id]['raw_epochs'].crop(tmin=0.1, tmax=0.7).get_data(copy=False)[train_indices[78:]]\n",
        "    sub_val_data[sub_id][f'y_train{str(f)}'] = sub_data[sub_id]['raw_labels'][train_indices[78:]]\n",
        "    sub_val_data[sub_id][f'x_norm_train_power2d{str(f)}'] = convert_2d(normalize(time_freq(sub_val_data[sub_id][f'x_train{str(f)}'])))\n",
        "\n",
        "  np.random.shuffle(train_indices)\n",
        "  np.random.shuffle(test_indices)\n",
        "  X_test = sub_data[sub_id]['raw_epochs'].crop(tmin=0.1, tmax=0.7).get_data(copy=False)[test_indices]\n",
        "  y_test = sub_data[sub_id]['raw_labels'][test_indices]\n",
        "  X_train = sub_data[sub_id]['raw_epochs'][train_indices]\n",
        "  y_train = sub_data[sub_id]['raw_labels'][train_indices]\n",
        "  x_train_power = time_freq(X_train)\n",
        "  x_test_power = time_freq(X_test)\n",
        "  sub_data_processed[sub_id] = {\n",
        "      'x_train_power': time_freq(X_train),\n",
        "      'x_test_power': time_freq(X_test),\n",
        "      'x_train_2d': convert_2d_ts(normalize(X_train)),\n",
        "      'y_train': y_train,\n",
        "      'x_test_2d': convert_2d_ts(normalize(X_test)),\n",
        "      'y_test':  y_test,\n",
        "      'x_norm_train_power2d': convert_2d(normalize(x_train_power)),\n",
        "      'x_norm_test_power2d': convert_2d(normalize(x_test_power))\n",
        "  }\n",
        "  #Saving time-series, time-freq data and labels for test & training\n",
        "  #sub_data_processed[sub_id]['x_train_power'] = time_freq(X_train)\n",
        "  #sub_data_processed[sub_id]['x_test_power'] = time_freq(X_test)\n",
        "  #sub_data_processed[sub_id]['x_train_2d'] = convert_2d_ts(normalize(X_train))\n",
        "  #sub_data_processed[sub_id]['y_train'] = y_train\n",
        "  #sub_data_processed[sub_id]['x_test_2d'] = convert_2d_ts(normalize(X_test))\n",
        "  #sub_data_processed[sub_id]['y_test'] = y_test\n",
        "  #Normalizing training & test data and reshaping from 4D to 2D\n",
        "  #sub_data_processed[sub_id]['x_norm_train_power2d'] = convert_2d(normalize(sub_data[sub_id]['x_train_power']))\n",
        "  #sub_data_processed[sub_id]['x_norm_test_power2d'] = convert_2d(normalize(sub_data[sub_id]['x_test_power']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preparing training and test datasets for each subject\n",
        "from mne.time_frequency import tfr_array_morlet\n",
        "s = ['23','24','25','26','28','30','31','33','34','35','36','37','41','42']\n",
        "n_trials = 35\n",
        "c_trials = 125\n",
        "i_trials = 35\n",
        "fold = 4\n",
        "sub_val_data = {}\n",
        "sub_data_processed = {}\n",
        "for sub_id in s:\n",
        "  # Discard extra kick/walk trials in neutral ones\n",
        "  neutral_kick_indices, neutral_walk_indices = get_trial_idx(np.where(sub_data[sub_id]['neutral_labels'] == 0)[0],np.where(sub_data[sub_id]['neutral_labels'] == 1)[0],35)\n",
        "  #Discard extra kick/walk trials in incongruent ones\n",
        "  incong_kick_indices, incong_walk_indices = get_trial_idx(np.where(sub_data[sub_id]['incong_labels'] == 0)[0],np.where(sub_data[sub_id]['incong_labels'] == 1)[0],35)\n",
        "  #Discard extra kick/walk trials in congruent ones\n",
        "  cong_kick_indices, cong_walk_indices = get_trial_idx(np.where(sub_data[sub_id]['cong_labels'] == 0)[0],np.where(sub_data[sub_id]['cong_labels'] == 1)[0],125)\n",
        "\n",
        "  #Find the trial indices to split into training and test sets (80%-20%)\n",
        "  train_indices = []\n",
        "  test_indices = []\n",
        "  np.random.shuffle(neutral_kick_indices)\n",
        "  np.random.shuffle(neutral_walk_indices)\n",
        "  np.random.shuffle(incong_kick_indices)\n",
        "  np.random.shuffle(incong_walk_indices)\n",
        "  np.random.shuffle(cong_kick_indices)\n",
        "  np.random.shuffle(cong_walk_indices)\n",
        "  train_indices = np.concatenate([neutral_kick_indices[:28],neutral_walk_indices[:28],incong_kick_indices[:28],incong_walk_indices[:28],cong_kick_indices[:100],cong_walk_indices[:100]])\n",
        "  test_indices = np.concatenate([neutral_kick_indices[28:],neutral_walk_indices[28:],incong_kick_indices[28:],incong_walk_indices[28:],cong_kick_indices[100:],cong_walk_indices[100:]])\n",
        "  sub_val_data[sub_id] = {\n",
        "        'x_val0': [],\n",
        "        'x_val1': [],\n",
        "        'x_val2': [],\n",
        "        'x_val3': [],\n",
        "        'y_val0': [],\n",
        "        'y_val1': [],\n",
        "        'y_val2': [],\n",
        "        'y_val3': [],\n",
        "        'x_norm_val_power2d0': [],\n",
        "        'x_norm_val_power2d1': [],\n",
        "        'x_norm_val_power2d2': [],\n",
        "        'x_norm_val_power2d3': [],\n",
        "        'x_norm_train_power2d0': [],\n",
        "        'x_norm_train_power2d1': [],\n",
        "        'x_norm_train_power2d2': [],\n",
        "        'x_norm_train_power2d3': [],\n",
        "        'y_train0': [],\n",
        "        'y_train1': [],\n",
        "        'y_train2': [],\n",
        "        'y_train3': [],\n",
        "        'x_train0':[],\n",
        "        'x_train1':[],\n",
        "        'x_train2':[],\n",
        "        'x_train3':[]\n",
        "    }\n",
        "  for f in range(fold):\n",
        "    np.random.shuffle(neutral_kick_indices)\n",
        "    np.random.shuffle(neutral_walk_indices)\n",
        "    np.random.shuffle(incong_kick_indices)\n",
        "    np.random.shuffle(incong_walk_indices)\n",
        "    np.random.shuffle(cong_kick_indices)\n",
        "    np.random.shuffle(cong_walk_indices)\n",
        "    #Extract training and validation data (time-series), from 100ms - 700ms\n",
        "    val_indices = np.concatenate([neutral_kick_indices[:7],neutral_walk_indices[:7],incong_kick_indices[:7],incong_walk_indices[:7],cong_kick_indices[:25],cong_walk_indices[:25]])\n",
        "    train_val_indices = np.concatenate([neutral_kick_indices[7:28],neutral_walk_indices[7:28],incong_kick_indices[7:28],incong_walk_indices[7:28],cong_kick_indices[25:100],cong_walk_indices[25:100]])\n",
        "    sub_val_data[sub_id][f'x_val{str(f)}'] = sub_data[sub_id]['raw_epochs'].crop(tmin=0.1, tmax=0.7).get_data(copy=False)[val_indices]\n",
        "    sub_val_data[sub_id][f'y_val{str(f)}'] = sub_data[sub_id]['raw_labels'][val_indices]\n",
        "    sub_val_data[sub_id][f'x_norm_val_power2d{str(f)}'] = convert_2d(normalize(time_freq(sub_val_data[sub_id][f'x_val{str(f)}'])))\n",
        "    sub_val_data[sub_id][f'x_train{str(f)}'] = sub_data[sub_id]['raw_epochs'].crop(tmin=0.1, tmax=0.7).get_data(copy=False)[train_val_indices]\n",
        "    sub_val_data[sub_id][f'y_train{str(f)}'] = sub_data[sub_id]['raw_labels'][train_val_indices]\n",
        "    sub_val_data[sub_id][f'x_norm_train_power2d{str(f)}'] = convert_2d(normalize(time_freq(sub_val_data[sub_id][f'x_train{str(f)}'])))\n",
        "\n",
        "  X_test = sub_data[sub_id]['raw_epochs'].crop(tmin=0.1, tmax=0.7).get_data(copy=False)[test_indices]\n",
        "  y_test = sub_data[sub_id]['raw_labels'][test_indices]\n",
        "  X_train = sub_data[sub_id]['raw_epochs'].crop(tmin=0.1, tmax=0.7).get_data(copy=False)[train_indices]\n",
        "  y_train = sub_data[sub_id]['raw_labels'][train_indices]\n",
        "  x_train_power = time_freq(X_train)\n",
        "  x_test_power = time_freq(X_test)\n",
        "  sub_data_processed[sub_id] = {\n",
        "      'x_train_power': time_freq(X_train),\n",
        "      'x_test_power': time_freq(X_test),\n",
        "      'x_train_2d': convert_2d_ts(normalize(X_train)),\n",
        "      'y_train': y_train,\n",
        "      'x_test_2d': convert_2d_ts(normalize(X_test)),\n",
        "      'y_test':  y_test,\n",
        "      'x_norm_train_power2d': convert_2d(normalize(x_train_power)),\n",
        "      'x_norm_test_power2d': convert_2d(normalize(x_test_power))\n",
        "  }\n",
        "  #Saving time-series, time-freq data and labels for test & training\n",
        "  #sub_data_processed[sub_id]['x_train_power'] = time_freq(X_train)\n",
        "  #sub_data_processed[sub_id]['x_test_power'] = time_freq(X_test)\n",
        "  #sub_data_processed[sub_id]['x_train_2d'] = convert_2d_ts(normalize(X_train))\n",
        "  #sub_data_processed[sub_id]['y_train'] = y_train\n",
        "  #sub_data_processed[sub_id]['x_test_2d'] = convert_2d_ts(normalize(X_test))\n",
        "  #sub_data_processed[sub_id]['y_test'] = y_test\n",
        "  #Normalizing training & test data and reshaping from 4D to 2D\n",
        "  #sub_data_processed[sub_id]['x_norm_train_power2d'] = convert_2d(normalize(sub_data[sub_id]['x_train_power']))\n",
        "  #sub_data_processed[sub_id]['x_norm_test_power2d'] = convert_2d(normalize(sub_data[sub_id]['x_test_power']))"
      ],
      "metadata": {
        "id": "SJYjggURnBsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/sub_val_data.pkl', 'wb') as file:\n",
        "    pickle.dump(sub_val_data, file)"
      ],
      "metadata": {
        "id": "QggTYYQfTtBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/sub_data_processed.pkl', 'wb') as file:\n",
        "    pickle.dump(sub_data_processed, file)"
      ],
      "metadata": {
        "id": "WRO_VMizYQj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCzHkTb4yOIe",
        "outputId": "089ccd0c-d16d-4756-b1a9-0b7f3e14ec5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10.  13.  21.5 30. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preparing training and test datasets for each subject\n",
        "from mne.time_frequency import tfr_array_morlet\n",
        "\n",
        "n_trials = 35\n",
        "c_trials = 125\n",
        "i_trials = 35\n",
        "\n",
        "for sub_id in s:\n",
        "  # Discard extra kick/walk trials in neutral ones\n",
        "  neutral_kick_indices, neutral_walk_indices = get_trial_idx(np.where(sub_data[sub_id]['neutral_labels'] == 0)[0],np.where(sub_data[sub_id]['neutral_labels'] == 1)[0],35)\n",
        "  #Discard extra kick/walk trials in incongruent ones\n",
        "  incong_kick_indices, incong_walk_indices = get_trial_idx(np.where(sub_data[sub_id]['incong_labels'] == 0)[0],np.where(sub_data[sub_id]['incong_labels'] == 1)[0],35)\n",
        "  #Discard extra kick/walk trials in congruent ones\n",
        "  cong_kick_indices, cong_walk_indices = get_trial_idx(np.where(sub_data[sub_id]['cong_labels'] == 0)[0],np.where(sub_data[sub_id]['cong_labels'] == 1)[0],125)\n",
        "\n",
        "  #Find the trial indices to split into training and test sets (80%-20%)\n",
        "  train_indices = []\n",
        "  test_indices = []\n",
        "  train_indices = np.concatenate([neutral_kick_indices[:28],neutral_walk_indices[:28],incong_kick_indices[:28],incong_walk_indices[:28],cong_kick_indices[:100],cong_walk_indices[:100]])\n",
        "  test_indices = np.concatenate([neutral_kick_indices[28:],neutral_walk_indices[28:],incong_kick_indices[28:],incong_walk_indices[28:],cong_kick_indices[100:],cong_walk_indices[100:]])\n",
        "  np.random.shuffle(train_indices)\n",
        "  np.random.shuffle(test_indices)\n",
        "\n",
        "  #Extract training and test data (time-series), from 100ms - 700ms\n",
        "  X_train = sub_data[sub_id]['raw_epochs'].crop(tmin=0.1, tmax=0.7).get_data(copy=False)[train_indices]\n",
        "  y_train = sub_data[sub_id]['raw_labels'][train_indices]\n",
        "  X_test = sub_data[sub_id]['raw_epochs'].crop(tmin=0.1, tmax=0.7).get_data(copy=False)[test_indices]\n",
        "  y_test = sub_data[sub_id]['raw_labels'][test_indices]\n",
        "\n",
        "  #Saving time-series, time-freq data and labels for test & training\n",
        "  sub_data[sub_id]['x_train_power'] = time_freq(X_train)\n",
        "  sub_data[sub_id]['x_test_power'] = time_freq(X_test)\n",
        "  sub_data[sub_id]['x_train'] = X_train\n",
        "  sub_data[sub_id]['y_train'] = y_train\n",
        "  sub_data[sub_id]['x_test'] = X_test\n",
        "  sub_data[sub_id]['y_test'] = y_test\n",
        "\n",
        "  #Normalizing training & test data and reshaping from 4D to 2D\n",
        "  sub_data[sub_id]['x_norm_train2d'] = convert_2d(normalize(sub_data[sub_id]['x_train_power']))\n",
        "  sub_data[sub_id]['x_norm_test2d'] = convert_2d(normalize(sub_data[sub_id]['x_test_power']))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y_5W027hJvjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare dataset for Z2\n",
        "#s=['23','24','25','26','28','30','31','33','34','35','36','37','41','42']\n",
        "s=['23','24','25','26','28','30','31','33','34','35','37','41','42']\n",
        "all = {}\n",
        "for sub_id in s:\n",
        "  if sub_id == '23':\n",
        "    all['train_2d'] = np.concatenate((sub_data[sub_id]['x_norm_train2d'],sub_data[sub_id]['x_norm_test2d']),axis=0)\n",
        "    all['train_label'] = np.concatenate((sub_data[sub_id]['y_train'],sub_data[sub_id]['y_test']),axis=0)\n",
        "  else:\n",
        "    a = np.concatenate((sub_data[sub_id]['x_norm_train2d'],sub_data[sub_id]['x_norm_test2d']),axis=0)\n",
        "    all['train_2d'] = np.concatenate((all['train_2d'],a),axis=0)\n",
        "    b = np.concatenate((sub_data[sub_id]['y_train'],sub_data[sub_id]['y_test']),axis=0)\n",
        "    all['train_label'] = np.concatenate((all['train_label'],b),axis=0)\n",
        "all['test_2d'] = np.concatenate((sub_data['36']['x_norm_train2d'],sub_data['36']['x_norm_test2d']),axis=0)\n",
        "all['test_2d'].shape\n",
        "all['test_label'] = np.concatenate((sub_data['36']['y_train'],sub_data['36']['y_test']),axis=0)\n",
        "print(all['test_label'].shape)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r4fFGA8TL1Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare dataset for Z3\n",
        "s=['23','24','25','26','28','30','31','33','34','35','37','41','42']\n",
        "all_z3 = {}\n",
        "for sub_id in s:\n",
        "  if sub_id == '23':\n",
        "    all_z3['train_2d'] = np.concatenate((convert_2d(sub_data[sub_id]['x_train_power']),convert_2d(sub_data[sub_id]['x_test_power'])),axis=0)\n",
        "    all_z3['train_label'] = np.concatenate((sub_data[sub_id]['y_train'],sub_data[sub_id]['y_test']),axis=0)\n",
        "  else:\n",
        "    a = np.concatenate((convert_2d(sub_data[sub_id]['x_train_power']),convert_2d(sub_data[sub_id]['x_test_power'])),axis=0)\n",
        "    all_z3['train_2d'] = np.concatenate((all_z3['train_2d'],a),axis=0)\n",
        "    b = np.concatenate((sub_data[sub_id]['y_train'],sub_data[sub_id]['y_test']),axis=0)\n",
        "    all_z3['train_label'] = np.concatenate((all_z3['train_label'],b),axis=0)\n",
        "\n",
        "\n",
        "all_z3['test_2d'] = np.concatenate((convert_2d(sub_data['36']['x_train_power']),convert_2d(sub_data['36']['x_test_power'])),axis=0)\n",
        "all_z3['test_2d'].shape\n",
        "all_z3['test_label'] = np.concatenate((sub_data['36']['y_train'],sub_data['36']['y_test']),axis=0)\n",
        "print(all_z3['test_label'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-BE-vFycSak",
        "outputId": "11625ee9-b8b3-49c7-f4a8-2015104140f8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(390,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Accuracy, Precision, Sensitivity, Specificity\n",
        "import numpy as np\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    correct_predictions = np.sum(y_true == y_pred)\n",
        "    total_predictions = len(y_true)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "def precision(tp, fp):\n",
        "    return tp / float(tp + fp)\n",
        "\n",
        "def sensitivity(tp, fn):\n",
        "    return tp / float(tp + fn)\n",
        "\n",
        "def specificity(tn,fn):\n",
        "    return tn / float(fn + tn)\n",
        "\n",
        "def acc_report(y_true, y_pred):\n",
        "    labels = np.unique(np.concatenate((y_true, y_pred)))\n",
        "    a = accuracy_score(y_true,y_pred)\n",
        "    for label in labels:\n",
        "        tp = np.sum((y_true == label) & (y_pred == label))\n",
        "        fp = np.sum((y_true != label) & (y_pred == label))\n",
        "        fn = np.sum((y_true == label) & (y_pred != label))\n",
        "        tn = np.sum((y_true !=label) & (y_pred != label))\n",
        "        p = precision(tp, fp)\n",
        "        r = sensitivity(tp, fn)\n",
        "        spec = specificity(tn,fn)\n",
        "\n",
        "    return a, p, r, spec"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qO1iMYkG7f31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we3Hewvca85w",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title KNN Algorithm\n",
        "import numpy as np\n",
        "\n",
        "#Find the euclidean distance between the two vectors\n",
        "def euclidean_distance(v1, v2):\n",
        "    return np.sqrt(np.sum((v1 - v2) ** 2))\n",
        "\n",
        "#Find the manhattan distance between the two vectors\n",
        "def manhattan_distance(v1,v2):\n",
        "    #print(np.sum(np.abs(v1-v2)))\n",
        "    return np.sum(np.abs(v1-v2))\n",
        "#Algorithm that takes the training dataset, y_train for the training dataset, test dataset, hyperparameter of #neighboring clusters\n",
        "def knn(train, y_train, test, k):\n",
        "\n",
        "    distances = []\n",
        "\n",
        "    # Compute distance from test sample to all training samples\n",
        "    for i in range(len(y_train)):\n",
        "        dist = manhattan_distance(test, train[i])\n",
        "        distances.append((y_train[i], dist))\n",
        "    #print(len(distances))\n",
        "    #print(len(y_train))\n",
        "\n",
        "    # Sort by distance not label (x[0]), and get top k which is the hyperparameter\n",
        "    k_neighbors = sorted(distances, key=lambda x: x[1])[:k]\n",
        "    #print(k_neighbors)\n",
        "    # Extract y_train of the k nearest neighbors\n",
        "    k_y_train = [label for label, _ in k_neighbors]\n",
        "\n",
        "    # Majority vote\n",
        "    prediction = max(set(k_y_train), key=k_y_train.count)\n",
        "    #print(prediction)\n",
        "    #print(distances)\n",
        "    return prediction\n",
        "def predict(train, y_train, test, k):\n",
        "    predictions = []\n",
        "\n",
        "    for test in test:\n",
        "        pred_class = knn(train, y_train, test, k)\n",
        "        predictions.append(pred_class)\n",
        "\n",
        "    return np.array(predictions)\n",
        "#k = range(1,20,2) # Number of neighbors\n",
        "acc = {}\n",
        "a = 0\n",
        "#s = [22,23,24,25,26,28,30,31,33,34,35,36,37,41,42]\n",
        "s=['25']\n",
        "for sub_id in s:\n",
        "  sub_str = str(sub_id)\n",
        "  test_predictions = predict(sub_data[sub_str]['x_norm_train2d'], sub_data[sub_str]['y_train'], sub_data[sub_str]['x_norm_test2d'] , k=1)\n",
        "  # Evaluate accuracy\n",
        "  accuracy = np.mean(test_predictions == sub_data[sub_str]['y_test'])\n",
        "  print(\"%s - Accuracy:\"%sub_str,accuracy)\n",
        "  acc[a] = accuracy\n",
        "  a+=1\n",
        "  sub_data[sub_id]['knn_tf'] = acc_report(test_predictions, sub_data[sub_id]['y_test'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVJwNWsigzwc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title KNN Cross-Validation\n",
        "def split_dataset(X, y, num_folds):\n",
        "    fold_size = len(X) // num_folds\n",
        "    X_folds = [X[i * fold_size:(i + 1) * fold_size] for i in range(num_folds)]\n",
        "    y_folds = [y[i * fold_size:(i + 1) * fold_size] for i in range(num_folds)]\n",
        "    return X_folds, y_folds\n",
        "\n",
        "def cross_validate_knn(X, y, k_values, num_folds=6):\n",
        "    X_folds, y_folds = split_dataset(X, y, num_folds)\n",
        "    k_scores = {}\n",
        "\n",
        "    for k in k_values:\n",
        "        accuracies = []\n",
        "\n",
        "        for i in range(num_folds):\n",
        "            # Prepare training and testing sets\n",
        "            X_train = np.concatenate([X_folds[j] for j in range(num_folds) if j != i])\n",
        "            y_train = np.concatenate([y_folds[j] for j in range(num_folds) if j != i])\n",
        "            X_test = X_folds[i]\n",
        "            y_test = y_folds[i]\n",
        "\n",
        "            # Reshape the data for kNN\n",
        "            #nsamples, nx, ny = X_train.shape\n",
        "            #X_train_2d = X_train.reshape((nsamples, nx*ny))\n",
        "            #nsamples, nx, ny = X_test.shape\n",
        "            #X_test_2d = X_test.reshape((nsamples, nx*ny))\n",
        "\n",
        "            # Train and predict with kNN\n",
        "            predictions = predict(X_train, y_train, X_test, k)\n",
        "            accuracy = np.mean(predictions == y_test)\n",
        "            accuracies.append(accuracy)\n",
        "        k_scores[k] = np.mean(accuracies)\n",
        "        print(k_scores[k])\n",
        "    return k_scores\n",
        "\n",
        "\n",
        "# Define the range of k values to test\n",
        "k_values = range(1,19,2)\n",
        "k_scores = cross_validate_knn(sub_data[sub_str]['x_norm_train2d'], sub_data[sub_str]['y_train'], k_values)\n",
        "\n",
        "# Find the best k value\n",
        "best_k = max(k_scores, key=k_scores.get)\n",
        "print(\"Best k:\",best_k, \"with accuracy:\", k_scores[best_k])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VwpHvNXwlGpE"
      },
      "outputs": [],
      "source": [
        "#@title Visualizing KNN Validation and Training for different K values\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(acc.keys(),acc.values())\n",
        "plt.plot(k_scores.keys(),k_scores.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH1ClntX6ZdE"
      },
      "outputs": [],
      "source": [
        "#@title Principal Component Analysis (PCA)\n",
        "import numpy as np\n",
        "class PCA:\n",
        "  def __init__(self,var_exp):\n",
        "    self.var_exp = var_exp\n",
        "    self.principal_components = None\n",
        "    self.mean = None\n",
        "    self.eigenvalues = None\n",
        "    self.eigenvectors = None\n",
        "  def fit(self, X):\n",
        "    self.mean = np.mean(X, axis=0)\n",
        "    X = X - self.mean\n",
        "    cov = np.cov(X.T)\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
        "    eigenvectors = eigenvectors.T\n",
        "    indices = np.argsort(abs(eigenvalues))[::-1]\n",
        "    eigenvalues = eigenvalues[indices]\n",
        "    eigenvectors = eigenvectors[indices]\n",
        "    tot, pve = 0, 0\n",
        "    for j in range(len(X[0,:])):\n",
        "      for i in range(234):\n",
        "        tot+= X[i][j]**2\n",
        "    for m in range(234):\n",
        "      for i in range(234):\n",
        "        pve+=(np.dot(X[i].T,eigenvectors[m]))**2\n",
        "      if np.real(pve/tot) > self.var_exp:\n",
        "        print(m)\n",
        "        break\n",
        "    self.principal_components = eigenvectors[0:m]\n",
        "    self.eigenvalues = eigenvalues[0:m]\n",
        "    self.eigenvectors = eigenvectors\n",
        "  def transform(self,X):\n",
        "    X = X - self.mean\n",
        "    return np.dot(X,self.principal_components.T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dimensionality Reduction with PCA for each subject followed by LDA\n",
        "s1 = ['23']\n",
        "for sub_id in s1:\n",
        "  pca = PCA(0.90)\n",
        "  pca.fit(sub_data_processed[sub_id]['x_norm_train_power2d'])\n",
        "  sub_data_processed[sub_id]['train_pca'] = np.real(pca.transform(sub_data_processed[sub_id]['x_norm_train_power2d']))\n",
        "  sub_data_processed[sub_id]['test_pca'] = np.real(pca.transform(sub_data_processed[sub_id]['x_norm_test_power2d']))\n",
        "  num_pc = len(pca.principal_components)\n",
        "  sub_data_processed[sub_id]['num_pc'] = num_pc\n",
        "#LDA for each subject\n",
        "for sub_id in s1:\n",
        "  lda = LDA(2,0)\n",
        "  lda.fit(sub_data_processed[sub_id]['train_pca'],sub_data_processed[sub_id]['y_train'])\n",
        "  sub_data_processed[sub_id]['train_lda'] = lda.transform(sub_data_processed[sub_id]['train_pca'])\n",
        "  sub_data_processed[sub_id]['test_lda'] = lda.transform(sub_data_processed[sub_id]['test_pca'])\n",
        "  sub_data_processed[sub_id]['conf_matrix'],sub_data_processed[sub_id]['lda_acc'],sub_data_processed[sub_id]['report']= nearest_centroid_classifier(sub_data_processed[sub_id]['train_lda'],sub_data_processed[sub_id]['y_train'],sub_data_processed[sub_id]['y_test'],sub_data_processed[sub_id]['test_lda'])"
      ],
      "metadata": {
        "id": "ikMzcpKHpl3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEZFel5gyVnk"
      },
      "outputs": [],
      "source": [
        "#@title Linear Support Vector Machine (SVM) Algorithm\n",
        "#The idea is to have a linear model, such that:\n",
        "#For c1: w*xi-b >= 1; For c2: w*xi-b <= -1 => For class label yi: yi(w*xi-b) >= 1\n",
        "#Come up with w (weights) and b (bias) by cost function and gradient descent\n",
        "#Hinge Loss: l = max(0,1-yi(w*xi-b))\n",
        "#Maximize the margin between two classes: 2/||w|| --> minimize the magnitude\n",
        "#Regularization: J = lambda*||w||^2 + 1/n(sum(hinge_loss)) --> from i to n\n",
        "#Gradients for w: if yi*f(x) >= 1: dJ_i/dw_k = 2*lambda*w_k / else: dJ_i/dw_k = 2*lambda*w_k - yi*xj\n",
        "#Gradients for b: if yi*f(x) >= 1: dJ_i/db = 0 else: dJ_i/db = yi\n",
        "#Update rule: w = w - alpha*dw / b = b - alpha*db; where alpha is the learning rate\n",
        "import numpy as np\n",
        "class SVM:\n",
        "  def __init__(self,lr = 0.001,lambda_param=0.01,n_iters=1000):\n",
        "    self.lr = lr\n",
        "    self.lambda_param = lambda_param\n",
        "    self.n_iters = n_iters\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "  def fit(self,X,y):\n",
        "    #since y = 0,1 we should convert to -1,1\n",
        "    y_ = np.where(y <= 0, -1, 1)\n",
        "    n_samples, n_features = X.shape\n",
        "    self.w = np.zeros(n_features)\n",
        "    self.b = 0\n",
        "    for _ in range(self.n_iters):\n",
        "      for idx, x_i in enumerate(X):\n",
        "        if y_[idx] * (np.dot(x_i,self.w)-self.b) >= 1:\n",
        "          self.w -= self.lr*(2*self.lambda_param*self.w)\n",
        "        else:\n",
        "          self.w -= self.lr*(2*self.lambda_param*self.w - np.dot(x_i,y_[idx]))\n",
        "          self.b -= self.lr*y_[idx]\n",
        "  def predict(self,X):\n",
        "    linear_output = np.dot(X,self.w) - self.b\n",
        "    return np.sign(linear_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sub_id in s:\n",
        "  svm = SVM()\n",
        "  svm.fit(sub_data_processed_pca[sub_id]['train_pca'],sub_data_processed_pca[sub_id]['y_train'])\n",
        "  labels = svm.predict(sub_data_processed_pca[sub_id]['test_pca'])\n",
        "  labels[np.where(labels==-1)] = 0\n",
        "  sub_data_processed_pca[sub_id]['svm_acc'] = acc_report(labels,sub_data_processed_pca[sub_id]['y_test'])"
      ],
      "metadata": {
        "id": "HBWOK6ZX898c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For Test-Training\n",
        "for sub_id in s:\n",
        "  pca = PCA(0.90)\n",
        "  pca.fit(sub_data_processed[sub_id]['x_norm_train2d'])\n",
        "  sub_data_processed[sub_id]['train_pca']=np.real(pca.transform(sub_data_processed[sub_id]['x_norm_train_power2d']))\n",
        "  sub_data_processed[sub_id]['test_pca']=np.real(pca.transform(sub_data_processed[sub_id]['x_norm_test_power2d']))"
      ],
      "metadata": {
        "id": "_3m7dMsyY7bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sub_id in s1:\n",
        "  lda = LDA(2,0)\n",
        "  lda.fit(sub_data_processed[sub_id]['train_pca'],sub_data_processed[sub_id]['y_train'])\n",
        "  sub_data_processed[sub_id]['train_lda'] = lda.transform(sub_data_processed[sub_id]['train_pca'])\n",
        "  sub_data_processed[sub_id]['test_lda'] = lda.transform(sub_data_processed[sub_id]['test_pca'])\n",
        "  sub_data_processed[sub_id]['conf_matrix'],sub_data_processed[sub_id]['lda_acc'],sub_data_processed[sub_id]['report']= nearest_centroid_classifier(sub_data_processed[sub_id]['train_lda'],sub_data_processed[sub_id]['y_train'],sub_data_processed[sub_id]['y_test'],sub_data_processed[sub_id]['test_lda'])"
      ],
      "metadata": {
        "id": "2hF331osv9lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59QoJCfyjeM-"
      },
      "outputs": [],
      "source": [
        "#@title Linear Discriminant Analysis (LDA) for Each Subject\n",
        "import numpy as np\n",
        "class LDA:\n",
        "  def __init__(self,n_components,shrinkage=0):\n",
        "    self.n_components = n_components\n",
        "    self.linear_discriminants = None\n",
        "    self.shrinkage = shrinkage\n",
        "  def fit(self, X, y):\n",
        "    n_features = X.shape[1]\n",
        "    class_labels = np.unique(y)\n",
        "    mean_overall = np.mean(X,axis=0)\n",
        "    print(self.shrinkage)\n",
        "    S_W = self.shrinkage*((X - mean_overall).T.dot(X - mean_overall))\n",
        "    S_B = np.zeros((n_features,n_features))\n",
        "    for c in class_labels:\n",
        "      X_c = X[y==c]\n",
        "      mean_c = np.mean(X_c, axis = 0)\n",
        "      S_W += (1-self.shrinkage)*((X_c - mean_c).T.dot(X_c - mean_c))\n",
        "      n_c = X_c.shape[0]\n",
        "      mean_diff = (mean_c - mean_overall).reshape(n_features,1)\n",
        "      S_B += n_c * (mean_diff).dot(mean_diff.T)\n",
        "    A = np.linalg.inv(S_W).dot(S_B)\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(A)\n",
        "    eigenvectors = eigenvectors.T\n",
        "    indices = np.argsort(abs(eigenvalues))[::-1]\n",
        "    eigenvalues = eigenvalues[indices]\n",
        "    eigenvectors = eigenvectors[indices]\n",
        "    self.linear_discriminants = eigenvectors[0:self.n_components]\n",
        "    sub_data[sub_id]['lda_weight'] = self.linear_discriminants[0]\n",
        "  def transform(self,X):\n",
        "    return np.dot(X,self.linear_discriminants.T)\n",
        "#x1 = sub_data[sub_id]['train_lda'][:,0]\n",
        "#x2 = sub_data[sub_id]['train_lda'][:,1]\n",
        "#plt.scatter(x1,x2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iijR5m8BvKa-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title LDA Accuracy Check - Centroid - Each Subject\n",
        "def nearest_centroid_classifier(X_train, y_train, y_test, X_test):\n",
        "    \"\"\"\n",
        "    Nearest Centroid Classifier\n",
        "    :param X_train: numpy.ndarray, Training data projected into LDA space\n",
        "    :param y_train: numpy.ndarray, Training labels\n",
        "    :param X_test: numpy.ndarray, Test data projected into LDA space\n",
        "    :return: Predicted labels for the test data\n",
        "    \"\"\"\n",
        "    class_labels = np.unique(y_train)\n",
        "    centroids = np.array([np.mean(X_train[y_train == cl], axis=0) for cl in class_labels])\n",
        "\n",
        "    # Find the nearest centroid for each test sample\n",
        "    distances = np.sqrt(((X_test - centroids[:, np.newaxis])**2).sum(axis=2))\n",
        "    nearest = np.argmin(distances, axis=0)\n",
        "\n",
        "    predictions = class_labels[nearest]\n",
        "    lda_accuracy = accuracy_score(y_test, predictions)\n",
        "    print(sub_id,f\"LDA Accuracy: {lda_accuracy}\")\n",
        "    accuracy = acc_report(y_test, predictions)\n",
        "    #print(\"Accuracy:\", accuracy)\n",
        "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
        "    print(\"Classification Report:\\n\", classification_rep)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LDA Accuracy Check - Probability Based\n",
        "def lda_predict_proba(X,w):\n",
        "    # X is the input data (features) for which you want to predict probabilities\n",
        "    # w is the weight vector obtained from LDA\n",
        "    # w0 is the intercept term obtained from LDA\n",
        "\n",
        "    # Calculate the discriminant scores for each class\n",
        "    #class_means = [np.mean(np.dot(X[y == 0],w)) for c in np.unique(y)]\n",
        "    #direction = 1 if class_means[1] > class_means[0] else -1\n",
        "    scores = np.dot(X, w)\n",
        "    #print(scores)\n",
        "    # Calculate class probabilities using the sigmoid function\n",
        "    #if direction == -1:\n",
        "      #scores=-scores\n",
        "    probabilities = np.exp(scores) / (1 + np.exp(scores))\n",
        "    return probabilities\n",
        "\n",
        "def lda_predict(X,w,threshold=0.5):\n",
        "    # X is the input data (features) for which you want to make predictions\n",
        "    # w is the weight vector obtained from LDA\n",
        "    # w0 is the intercept term obtained from LDA\n",
        "    # threshold is the decision threshold for converting probabilities to class labels\n",
        "    # Get class probabilities\n",
        "    probabilities = lda_predict_proba(X,w)\n",
        "\n",
        "    # Make predictions based on the decision threshold\n",
        "    predictions = (probabilities > threshold).astype(int)\n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "    confusion_mat = confusion_matrix(sub_data[sub_id]['y_test'], predictions)\n",
        "    print(sub_id,accuracy_score(sub_data[sub_id]['y_test'], predictions))\n",
        "    print(confusion_mat)\n",
        "    return predictions\n",
        "for sub_id in s1:\n",
        "  predictions = lda_predict(sub_data[sub_id]['test_pca'],sub_data[sub_id]['lda_weight'])\n",
        "\n"
      ],
      "metadata": {
        "id": "N9VMLkK19t3n",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}